{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will focus on processing of text data and the results of the clustering from the previous section for modeling and testing in the next section.\n",
    "\n",
    "First, I load the necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nickj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nickj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import math\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a dictionary linking each movie ID to its reviews, and also initialize the Porter Stemmer, a text processing tool which reduces similar words to a shared base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_reviews_clean.csv')\n",
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "dict_keys = df['movieId']\n",
    "dict_items = df['review_text']\n",
    "print(len(dict_keys))\n",
    "data = dict()\n",
    "\n",
    "print(dict_items[0])\n",
    "for i in range(len(dict_keys)):\n",
    "    print(dict_keys[i])\n",
    "    data[dict_keys[i]] = dict_items[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I split each movie's reviews into individual words, convert them to lower case, remove any commonly used stopwords, and user the Porter Stemmer to stem the words, saving the result using Pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalwords = []\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for key in data.keys():\n",
    "    print(key)\n",
    "    words = nltk.word_tokenize(data[key])\n",
    "\n",
    "    words_no_p = []\n",
    "\n",
    "    for w in words:\n",
    "        if w.isalpha():\n",
    "            words_no_p.append(w.lower())\n",
    "    #print(words_no_p)\n",
    "    clean_words = []\n",
    "    for w in words_no_p:\n",
    "        if w not in nltk.corpus.stopwords.words('english'):\n",
    "            clean_words.append(w)\n",
    "    for i in range(len(clean_words)):\n",
    "        clean_words[i] = porter.stem(clean_words[i])\n",
    "    totalwords.append(clean_words)\n",
    "\n",
    "outfile = open('totalwords.pkl','wb')\n",
    "pkl.dump(totalwords,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then create a list of strings to represent the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9985\n"
     ]
    }
   ],
   "source": [
    "totalwords = pkl.load(open('totalwords.pkl','rb'))\n",
    "\n",
    "newlist = []\n",
    "for x in totalwords:\n",
    "    text = \"\"\n",
    "    for word in x:\n",
    "        text = text + word + \" \"\n",
    "    newlist.append(text)\n",
    "\n",
    "print(len(newlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create 1grams (single words) and bigrams (pairs of words) from the data. I use the Tf-Idf (term frequency/inverse document frequency) method to find the most relevant terms for the creation of the 1grams and bigrams.\n",
    "\n",
    "Some of this code is adapted from code that I found on the internet while researching this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Words head : \n",
      "           term        rank\n",
      "18692     film  716.570068\n",
      "36320     movi  640.840515\n",
      "38839      one  351.436432\n",
      "31271     like  290.370239\n",
      "9040   charact  258.497742\n",
      "54558     time  234.471924\n",
      "51778    stori  231.685501\n",
      "21784     good  222.329208\n",
      "47820      see  218.288284\n",
      "32773     make  218.128036\n",
      "31943     love  205.090851\n",
      "21095      get  197.714996\n",
      "59085    watch  196.089096\n",
      "22207    great  189.719666\n",
      "44139   realli  185.801620\n",
      "47202    scene  182.003174\n",
      "60402    would  180.950806\n",
      "59372     well  178.284592\n",
      "40668    peopl  177.449936\n",
      "17361     even  174.749695\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None,ngram_range = (1,1)) \n",
    "X1 = vectorizer.fit_transform(newlist)  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None,ngram_range = (1,1)) \n",
    "X2 = vectorizer.fit_transform(newlist) \n",
    "X2 = X2.astype('float32')\n",
    "scores = (X2.toarray()) \n",
    "#print(\"\\n\\nScores : \\n\", scores) \n",
    "\n",
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0,col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank']) \n",
    "words_1gram = (ranking.sort_values('rank', ascending = False)) \n",
    "print (\"\\n\\nWords head : \\n\", words_1gram.head(20)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('1grams.pkl','wb')\n",
    "pkl.dump(words_1gram.head(2500),outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Words head : \n",
      "                   term      rank\n",
      "198525        one best  4.708863\n",
      "306868      watch movi  3.825894\n",
      "105626      first time  3.475363\n",
      "192430        new york  3.242450\n",
      "89722        ever seen  3.169294\n",
      "89098      even though  3.108515\n",
      "185926       movi like  3.061846\n",
      "167387       love movi  2.995861\n",
      "319299        year old  2.961021\n",
      "185519       movi ever  2.946520\n",
      "120126       good movi  2.885079\n",
      "167617      love stori  2.878795\n",
      "2560       action movi  2.849945\n",
      "264265  special effect  2.830976\n",
      "306682      watch film  2.739706\n",
      "186108        movi one  2.719745\n",
      "122453      great movi  2.687935\n",
      "247917        see movi  2.671350\n",
      "95975        fall love  2.664104\n",
      "198854     one favorit  2.654391\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None,ngram_range = (2,2)) \n",
    "X1 = vectorizer.fit_transform(newlist[0:1000])  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "#print(\"\\n\\nFeatures : \\n\", features) \n",
    "#print(\"\\n\\nX1 : \\n\", X1.toarray()) \n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None,ngram_range = (2,2)) \n",
    "X2 = vectorizer.fit_transform(newlist[0:1000]) \n",
    "X2 = X2.astype('float32')\n",
    "scores = (X2.toarray()) \n",
    "#print(\"\\n\\nScores : \\n\", scores) \n",
    "\n",
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0,col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank']) \n",
    "words_bigram = (ranking.sort_values('rank', ascending = False)) \n",
    "print (\"\\n\\nWords head : \\n\", words_bigram.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('bigram1.pkl','wb')\n",
    "pkl.dump(words_bigram.head(5000),outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to extract the bigrams over several iterations due to memory constraints. I then combined all the separate bigram files with the 1grams file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "the1grams = pkl.load(open(\"1grams.pkl\",'rb'))\n",
    "bigrams1 = pkl.load(open(\"bigram\" + str(1) + \".pkl\",'rb'))\n",
    "for i in range(2,11):\n",
    "    bigrams1 = pd.merge(pkl.load(open(\"bigram\" + str(i) + \".pkl\",'rb')),bigrams1,how='inner',on='term')\n",
    "\n",
    "\n",
    "the1grams = pd.merge(the1grams,bigrams1,how='outer',on='term')\n",
    "the1grams.to_csv('total_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the terms to the file total_words.csv, I then removed any irrelevant terms by hand, keeping only the ones which were relevant dsecriptors of the movie and would be valuable features. eg. I removed \"good movie\" because it is nonspecific and offers no information, but kept \"action scene\" because an action scene is a feature that some movie watchers may find more desirable than others. The final list of terms is saved as total_words_reduced.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then construct a new dataframe, and store the count of each relevant word/bigram feature in the reviews for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(columns=['movieId','Word','Count'])\n",
    "totalwords = []\n",
    "words_used = pd.read_csv('total_words_reduced.csv')['term'].to_list()\n",
    "\n",
    "for key in data.keys():\n",
    "    print(key)\n",
    "    print(df_new.shape)\n",
    "    words = nltk.word_tokenize(data[key])\n",
    "\n",
    "    words_no_p = []\n",
    "\n",
    "    for w in words:\n",
    "        if w.isalpha():\n",
    "            words_no_p.append(w.lower())\n",
    "    clean_words = []\n",
    "    for w in words_no_p:\n",
    "        if w not in nltk.corpus.stopwords.words('english'):\n",
    "            clean_words.append(w)\n",
    "            \n",
    "    for i in range(len(clean_words)):\n",
    "        clean_words[i] = porter.stem(clean_words[i])\n",
    "        \n",
    "    bigrams = []\n",
    "    for i in range(len(clean_words)-1):\n",
    "        bigrams.append(clean_words[i] + \" \" + clean_words[i+1])\n",
    "    c = Counter(clean_words)\n",
    "    bc = Counter(bigrams)\n",
    "    for k,v in c.items():\n",
    "        if k not in words_used:\n",
    "            continue\n",
    "        #k = porter.stem(k)\n",
    "        vals = {\"movieId\":key,\"Word\":k,\"Count\":v}\n",
    "        df_new = df_new.append(vals,ignore_index=True)\n",
    "        \n",
    "    for k,v in bc.items():\n",
    "        vals = {\"movieId\":key,\"Word\":k,\"Count\":v}\n",
    "        if k not in words_used:\n",
    "            continue\n",
    "        df_new = df_new.append(vals,ignore_index=True)\n",
    "\n",
    "df_new.to_csv('all_words_final.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having acquired the word features, I turn my attention back to the genre information. In order to make each cluster disjoint, I require a 'tie breaker' to determine which cluster a movie should belong to if it belongs to genres overlapping multiple clusters, the importance dict below fulfills this need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_dict = {'Documentary':17,'Family':16,'Horror':15,\"Comedy\":14,\"Animation\":13,\"Thriller\":12,\"Sci-Fi\":11,\"Action\":10,\"Drama\":9,\"Romance\":8,\"Fantasy\":7,\"Musical\":6,\"Mystery\":5,\"Adventure\":4,\"War\":3,\"Western\":2,\"Crime\":1,\"Film-Noir\":0,\"Biography\":-1,\"History\":-2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used to split all the movies into the appropriate clusters, determined in the previous notebook, and the appropriate superclusters (larger clusters that several clusters can belong to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(x):\n",
    "    clusters_dict = pkl.load(open('movie_clusters.pkl','rb'))[0]\n",
    "    #print(clusters_dict)\n",
    "    genres = x[11:82]\n",
    "    #print(genres)\n",
    "    genres_used = genres[genres == 1]\n",
    "    genres_dropped = set()\n",
    "    for genre in genres_used.index:\n",
    "        genre_split = genre.split('_')\n",
    "        for genre2 in genres_used.index:\n",
    "            if genre == genre2:\n",
    "                continue\n",
    "            genre2_split = genre2.split('_')\n",
    "            if (len(genre_split) == 2) and (len(genre2_split) == 2):\n",
    "                for i in range(len(genre_split)):\n",
    "                    for j in range(len(genre2_split)):\n",
    "                        if genre_split[i] == genre2_split[j]:\n",
    "                            if importance_dict[genre_split[1-i]] > importance_dict[genre2_split[1-j]]:\n",
    "                                genres_dropped.add(genre2)\n",
    "                            else:\n",
    "                                genres_dropped.add(genre)\n",
    "    #print(genres_dropped)\n",
    "    for genre in genres_dropped:\n",
    "        genres_used = genres_used.drop(genre)  \n",
    "    #print(genres_used) \n",
    "    #print(genres)\n",
    "    if len(genres_used) == 0:\n",
    "        return -1\n",
    "    determining_genre = genres_used.index[0]\n",
    "    split = []\n",
    "    for genre in genres_used.index:\n",
    "        if '_' in genre:\n",
    "            determining_genre = genre\n",
    "            split = genre.split('_')\n",
    "    \n",
    "    for genre in genres_used.index:\n",
    "        if '_' not in genre:\n",
    "            if len(split) != 0:\n",
    "                if (importance_dict[genre] > importance_dict[split[0]]) and (importance_dict[genre] > importance_dict[split[1]]):\n",
    "                    determining_genre = genre\n",
    "            else:\n",
    "                for genre2 in genres_used.index:\n",
    "                    if genre == genre2:\n",
    "                        continue\n",
    "                    if importance_dict[genre] > importance_dict[genre2]:\n",
    "                        determining_genre = genre\n",
    "                    else:\n",
    "                        determing_genre = genre2\n",
    "                        \n",
    "    return(clusters_dict[determining_genre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supercluster(x):\n",
    "    clusters_dict = pkl.load(open('movie_clusters.pkl','rb'))[0]\n",
    "    superclusters_dict = pkl.load(open('movie_clusters.pkl','rb'))[1]\n",
    "    super_clusters = {}\n",
    "    for key in clusters_dict.keys():\n",
    "        super_clusters[clusters_dict[key]] = superclusters_dict[key]\n",
    "    return(super_clusters[x['cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'title', 'age_rating', 'time_minutes', 'genres', 'imdb_score',\n",
      "       'imdb_votes', 'director', 'actors', 'language', 'movieId', 'Drama',\n",
      "       'Fantasy', 'War', 'Sci-Fi', 'Biography', 'Thriller', 'Action', 'Family',\n",
      "       'History', 'Animation', 'Mystery', 'Musical', 'Documentary', 'Crime',\n",
      "       'Romance', 'Comedy', 'Adventure', 'Film-Noir', 'Horror', 'Western',\n",
      "       'Drama_Fantasy', 'Drama_War', 'Drama_Sci-Fi', 'Drama_Biography',\n",
      "       'Drama_Thriller', 'Drama_Action', 'Drama_Family', 'Drama_History',\n",
      "       'Drama_Mystery', 'Drama_Crime', 'Drama_Romance', 'Drama_Comedy',\n",
      "       'Drama_Adventure', 'Drama_Horror', 'Fantasy_Sci-Fi', 'Fantasy_Action',\n",
      "       'Fantasy_Family', 'Fantasy_Animation', 'Fantasy_Romance',\n",
      "       'Fantasy_Comedy', 'Fantasy_Adventure', 'Fantasy_Horror',\n",
      "       'Sci-Fi_Thriller', 'Sci-Fi_Action', 'Sci-Fi_Comedy', 'Sci-Fi_Adventure',\n",
      "       'Sci-Fi_Horror', 'Thriller_Action', 'Thriller_Mystery',\n",
      "       'Thriller_Crime', 'Thriller_Romance', 'Thriller_Comedy',\n",
      "       'Thriller_Adventure', 'Thriller_Horror', 'Action_Crime',\n",
      "       'Action_Comedy', 'Action_Adventure', 'Family_Animation',\n",
      "       'Family_Comedy', 'Family_Adventure', 'Animation_Comedy',\n",
      "       'Animation_Adventure', 'Mystery_Crime', 'Mystery_Horror',\n",
      "       'Musical_Comedy', 'Crime_Romance', 'Crime_Comedy', 'Romance_Comedy',\n",
      "       'Romance_Adventure', 'Comedy_Adventure', 'Comedy_Horror', 'dates',\n",
      "       'decade', '1990s', '1970s', '1960s', '1980s', '1930s', '1940s', '1950s',\n",
      "       '1920s', '2000s', '1910s', '2010s', 'decade_cluster',\n",
      "       'decade_supercluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movies_everything.csv').iloc[:,1:]\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I call the functions to cluster the data, and combine the genre clusters with the decade information from earlier, to determine the final clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       17\n",
      "1       17\n",
      "2        8\n",
      "3        8\n",
      "4       17\n",
      "        ..\n",
      "9816    19\n",
      "9817    22\n",
      "9818    19\n",
      "9819    23\n",
      "9820     8\n",
      "Name: cluster, Length: 9821, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['cluster'] = df.apply(lambda x: get_cluster(x),axis=1)\n",
    "df['supercluster'] = df.apply(lambda x: get_supercluster(x),axis=1)\n",
    "df['combined_cluster'] = df.apply(lambda x: str(df['cluster']) + '_' + str(df['decade_cluster']))\n",
    "df['combined_supercluster'] = df.apply(lambda x: str(df['cluster']) + '_' + str(df['decade_supercluster']))\n",
    "df['combined_megacluster'] = df.apply(lambda x: str(df['supercluster']) + '_' + str(df['decade_supercluster']))\n",
    "print(df['cluster'])\n",
    "\n",
    "df.to_csv('movies_everything_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, I will use the model I have developed to recommend movies based on a single movie input, and test the functionality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
